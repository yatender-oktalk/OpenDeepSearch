#!/usr/bin/env python3
"""
Script 1: Generate Data and Store in Neo4j
- Fetches authentic WHO COVID-19 data from multiple government sources
- Generates realistic e-commerce data using LLM
- Stores everything in Neo4j with temporal relationships
"""

import os
import json
import requests
import pandas as pd
from neo4j import GraphDatabase
from datetime import datetime, timedelta
import openai
from typing import List, Dict
from dataclasses import dataclass
import random
import re

try:
    from bs4 import BeautifulSoup

    BEAUTIFULSOUP_AVAILABLE = True
except ImportError:
    print("‚ö†Ô∏è BeautifulSoup not available. Install with: pip install beautifulsoup4")
    BEAUTIFULSOUP_AVAILABLE = False

# Configuration
DATASET_NO = int(os.getenv("DATASET_NO", "10"))
USE_LLM_FOR_COVID = (
    os.getenv("USE_LLM_FOR_COVID", "false").lower() == "true"
)  # Pure LLM generation
USE_AUTHENTIC_SCRAPING = (
    os.getenv("USE_AUTHENTIC_SCRAPING", "true").lower() == "true"
)  # Scrape + LLM structure
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
NEO4J_URI = os.getenv("NEO4J_URI", "bolt://localhost:7687")
NEO4J_USER = os.getenv("NEO4J_USERNAME", "neo4j")
NEO4J_PASSWORD = os.getenv("NEO4J_PASSWORD")


@dataclass
class Event:
    entity_id: str
    event_type: str
    description: str
    timestamp: str
    domain: str  # 'covid' or 'ecommerce'
    location: str
    metadata: Dict


class DataGenerator:
    def __init__(self):
        self.neo4j_driver = GraphDatabase.driver(
            NEO4J_URI, auth=(NEO4J_USER, NEO4J_PASSWORD)
        )
        openai.api_key = OPENAI_API_KEY

        # Setup session for web scraping
        self.session = requests.Session()
        self.session.headers.update(
            {
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
            }
        )

        # Government COVID data sources
        self.covid_sources = {
            "CDC": {
                "url": "https://www.cdc.gov/museum/timeline/covid19.html",
                "target_events": 3,
                "keywords": [
                    "covid",
                    "coronavirus",
                    "pandemic",
                    "lockdown",
                    "who",
                    "cdc",
                    "declares",
                    "emergency",
                ],
            },
            "WHO": {
                "url": "https://www.who.int/emergencies/diseases/novel-coronavirus-2019",
                "target_events": 3,
                "keywords": [
                    "january",
                    "february",
                    "march",
                    "april",
                    "declared",
                    "emergency",
                    "pandemic",
                    "pheic",
                ],
            },
            "ECDC": {
                "url": "https://www.ecdc.europa.eu/en/covid-19",
                "target_events": 2,
                "keywords": [
                    "covid",
                    "outbreak",
                    "europe",
                    "surveillance",
                    "response",
                    "measures",
                ],
            },
            "Our_World_Data": {
                "url": "https://ourworldindata.org/coronavirus",
                "target_events": 2,
                "keywords": [
                    "data",
                    "statistics",
                    "global",
                    "vaccination",
                    "deaths",
                    "cases",
                ],
            },
        }

    def fetch_authentic_covid_data(self) -> List[Event]:
        """Generate COVID-19 data using LLM for realistic scenarios"""
        print(f"üì° Generating COVID-19 events using LLM...")

        try:
            target_count = DATASET_NO // 2

            prompt = f"""
            Generate exactly {target_count} realistic COVID-19 pandemic events from 2019-2023.
            Create a chronological progression showing the pandemic evolution.
            
            Include diverse event types:
            - Initial outbreak reports
            - WHO declarations and policy changes  
            - Country-specific lockdowns and restrictions
            - Vaccine developments and approvals
            - Variant discoveries
            - Milestone case/death counts
            
            Return JSON array with exactly {target_count} items:
            [
              {{
                "date": "YYYY-MM-DD",
                "event_type": "outbreak|declaration|lockdown|vaccine|variant|milestone",
                "location": "country or Global",
                "description": "detailed event description with context and numbers",
                "cases": number_of_cases,
                "deaths": number_of_deaths,
                "significance": "major|significant|moderate"
              }}
            ]
            
            Make events historically plausible with realistic progression from outbreak to vaccines.
            Include real countries and believable case numbers.
            """

            response = openai.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=[
                    {
                        "role": "system",
                        "content": "Generate realistic COVID-19 pandemic timeline data with accurate progression and plausible statistics.",
                    },
                    {"role": "user", "content": prompt},
                ],
                temperature=0.7,
                max_tokens=3000,
            )

            events_data = json.loads(response.choices[0].message.content)
            events_data = events_data[:target_count]  # Ensure exact count

            events = []
            for i, event_data in enumerate(events_data):
                event = Event(
                    entity_id=f"LLM_COVID_{i:04d}",
                    event_type=event_data.get("event_type", "unknown"),
                    description=event_data.get("description", "COVID-19 event"),
                    timestamp=event_data.get("date"),
                    domain="covid",
                    location=event_data.get("location", "Global"),
                    metadata={
                        "cases": int(event_data.get("cases", 0)),
                        "deaths": int(event_data.get("deaths", 0)),
                        "significance": event_data.get("significance", "moderate"),
                        "source": "LLM Generated COVID-19 Events",
                        "event_category": event_data.get("event_type"),
                    },
                )
                events.append(event)

            # Pad with fallback if needed
            if len(events) < target_count:
                fallback_events = self._fallback_covid_events(
                    target_count - len(events)
                )
                events.extend(fallback_events)

            print(f"‚úÖ LLM generated {len(events)} COVID events")
            print(f"   üìä Event types: {set(e.event_type for e in events)}")
            print(f"   üåç Locations: {set(e.location for e in events)}")
            return events

        except Exception as e:
            print(f"‚ö†Ô∏è LLM COVID generation failed: {e}")
            print(f"   Using verified historical timeline as fallback...")
            return self._fallback_covid_events(DATASET_NO // 2)

    def scrape_authentic_covid_sources(self, target_count: int) -> List[Event]:
        """Enhanced scraping from multiple government COVID sources"""
        print("üåê Scraping multiple authentic COVID-19 government sources...")

        if not BEAUTIFULSOUP_AVAILABLE:
            print("‚ö†Ô∏è BeautifulSoup not available for scraping. Using fallback.")
            return self._fallback_covid_events(target_count)

        all_events = []

        try:
            # Scrape from all government sources
            for source_name, source_config in self.covid_sources.items():
                print(f"   üì° Scraping {source_name}...")

                try:
                    content = self._scrape_government_source(source_name, source_config)
                    if content:
                        events = self._extract_events_with_llm(
                            content, source_name, source_config["target_events"]
                        )
                        all_events.extend(events)
                        print(f"   ‚úÖ {source_name}: {len(events)} events extracted")
                    else:
                        print(f"   ‚ö†Ô∏è {source_name}: No content extracted")

                except Exception as e:
                    print(f"   ‚ùå {source_name} failed: {e}")
                    continue

            # Remove duplicates and validate
            unique_events = self._remove_duplicate_events(all_events)
            validated_events = sorted(unique_events, key=lambda x: x.timestamp)

            print(
                f"   üìä Total scraped events: {len(validated_events)} (from {len(all_events)} raw)"
            )

            # Supplement with Our World in Data if needed
            if len(validated_events) < target_count:
                owid_events = self._supplement_with_owid_data(
                    target_count - len(validated_events)
                )
                validated_events.extend(owid_events)

            # Final fallback if still short
            if len(validated_events) < target_count:
                fallback_needed = target_count - len(validated_events)
                print(
                    f"   üìö Supplementing with {fallback_needed} verified timeline events..."
                )
                fallback_events = self._fallback_covid_events(fallback_needed)
                validated_events.extend(fallback_events)

            # Ensure exact target count
            final_events = validated_events[:target_count]

            print(
                f"‚úÖ Total COVID events: {len(final_events)} (target: {target_count})"
            )
            sources_used = set(
                e.metadata.get("source", "Unknown") for e in final_events
            )
            print(f"   üåê Sources used: {len(sources_used)} different sources")
            return final_events

        except Exception as e:
            print(f"‚ö†Ô∏è Enhanced scraping failed: {e}")
            print("   üìö Using full fallback timeline...")
            return self._fallback_covid_events(target_count)

    def _scrape_government_source(self, source_name: str, config: Dict) -> str:
        """Generic government source scraper with improved content filtering"""
        try:
            url = config["url"]
            keywords = config["keywords"]

            response = self.session.get(url, timeout=30)
            response.raise_for_status()

            soup = BeautifulSoup(response.content, "html.parser")

            # Remove unwanted elements
            for element in soup(
                ["script", "style", "nav", "header", "footer", "aside"]
            ):
                element.decompose()

            # Extract relevant content
            relevant_content = []

            for element in soup.find_all(
                ["p", "div", "li", "span", "article", "section"]
            ):
                text = element.get_text(strip=True)
                if text and 30 <= len(text) <= 400:  # Filter for substantive content
                    # Check if text contains relevant keywords and date patterns
                    text_lower = text.lower()
                    has_date = bool(
                        re.search(
                            r"\b(january|february|march|april|may|june|july|august|september|october|november|december|\d{1,2}[\/\-\.]\d{1,2}[\/\-\.]\d{4}|\d{4}[\/\-\.]\d{2}[\/\-\.]\d{2}|202[0-3])",
                            text_lower,
                        )
                    )
                    has_keywords = any(keyword in text_lower for keyword in keywords)

                    if has_date and has_keywords:
                        relevant_content.append(text)

            # Prioritize content by keyword density
            scored_content = []
            for text in relevant_content:
                score = sum(1 for keyword in keywords if keyword in text.lower())
                scored_content.append((score, text))

            # Sort by score and take top content
            scored_content.sort(key=lambda x: x[0], reverse=True)
            top_content = [text for score, text in scored_content[:15]]  # Top 15 pieces

            # Limit total content for LLM processing
            content = " ".join(top_content)[:2500]  # Max 2500 chars per source

            print(
                f"   ‚úÖ {source_name}: Extracted {len(content)} characters from {len(top_content)} pieces"
            )
            return content

        except Exception as e:
            print(f"   ‚ö†Ô∏è {source_name} scraping failed: {e}")
            return ""

    def _extract_events_with_llm(
        self, content: str, source: str, max_events: int
    ) -> List[Event]:
        """Use LLM to extract structured events with improved JSON parsing"""
        print(f"   ü§ñ Extracting events from {source} using LLM...")

        if not content.strip():
            print(f"   ‚ö†Ô∏è No content to extract from {source}")
            return []

        try:
            prompt = f"""
            Extract COVID-19 timeline events from this authentic government content from {source}.
            
            Content: {content}
            
            Extract exactly {max_events} significant COVID-19 events and return ONLY a valid JSON array.
            
            Each event must have these exact fields:
            - "date": "YYYY-MM-DD" format (estimate if needed, use 2020-2021 for most events)
            - "event_type": one of: "outbreak", "declaration", "lockdown", "policy", "milestone", "vaccine", "variant"
            - "location": specific country name or "Global"
            - "description": clear description (20-150 words)
            - "significance": one of: "major", "significant", "moderate"
            
            Requirements:
            1. Return ONLY the JSON array, no other text
            2. Extract events with clear dates and significance
            3. Focus on major milestones, declarations, policy changes
            4. Use realistic dates in 2020-2023 timeframe
            5. Make descriptions detailed and informative
            
            Example format:
            [
              {{
                "date": "2020-03-11",
                "event_type": "declaration",
                "location": "Global",
                "description": "WHO declares COVID-19 a pandemic after global spread reaches 114 countries with sustained community transmission",
                "significance": "major"
              }}
            ]
            
            Return exactly {max_events} events as valid JSON array only.
            """

            # Try LLM extraction with retry logic
            for attempt in range(2):  # Allow 1 retry
                try:
                    response = openai.chat.completions.create(
                        model="gpt-4",
                        messages=[
                            {
                                "role": "system",
                                "content": "You are a precise data extractor. Return only valid JSON arrays with COVID-19 events from authentic government sources. No additional text or explanations.",
                            },
                            {"role": "user", "content": prompt},
                        ],
                        temperature=0.1,  # Very low temperature for consistency
                        max_tokens=1500,
                    )

                    response_content = response.choices[0].message.content.strip()

                    # Clean the response to extract JSON
                    json_content = self._extract_json_from_response(response_content)

                    if json_content:
                        events_data = json.loads(json_content)

                        # Validate it's a list
                        if not isinstance(events_data, list):
                            print(
                                f"   ‚ö†Ô∏è LLM returned non-list for {source} (attempt {attempt + 1})"
                            )
                            continue

                        # Convert to Event objects
                        events = []
                        for i, event_data in enumerate(events_data[:max_events]):
                            if not isinstance(event_data, dict):
                                continue

                            required_fields = ["date", "description", "location"]
                            if not all(
                                field in event_data for field in required_fields
                            ):
                                print(
                                    f"   ‚ö†Ô∏è Skipping incomplete event {i} from {source}"
                                )
                                continue

                            # Validate date format
                            date_str = event_data["date"]
                            if not re.match(r"\d{4}-\d{2}-\d{2}", date_str):
                                print(
                                    f"   ‚ö†Ô∏è Invalid date format '{date_str}' in {source}"
                                )
                                continue

                            event = Event(
                                entity_id=f"SCRAPED_{source.replace(' ', '_').upper()}_{i:03d}",
                                event_type=event_data.get("event_type", "milestone"),
                                description=event_data["description"],
                                timestamp=date_str,
                                domain="covid",
                                location=event_data["location"],
                                metadata={
                                    "significance": event_data.get(
                                        "significance", "moderate"
                                    ),
                                    "source": f"Scraped from {source}",
                                    "extraction_method": "LLM_structured",
                                    "authentic_source": True,
                                    "attempt": attempt + 1,
                                },
                            )
                            events.append(event)

                        if events:
                            print(f"   ‚úÖ Extracted {len(events)} events from {source}")
                            return events
                        else:
                            print(
                                f"   ‚ö†Ô∏è No valid events extracted from {source} (attempt {attempt + 1})"
                            )
                    else:
                        print(
                            f"   ‚ö†Ô∏è Could not find JSON in LLM response from {source} (attempt {attempt + 1})"
                        )

                except json.JSONDecodeError as e:
                    print(
                        f"   ‚ö†Ô∏è JSON parsing failed for {source} (attempt {attempt + 1}): {e}"
                    )
                except Exception as e:
                    print(
                        f"   ‚ö†Ô∏è LLM extraction error for {source} (attempt {attempt + 1}): {e}"
                    )

            print(f"   ‚ùå Failed to extract events from {source} after 2 attempts")
            return []

        except Exception as e:
            print(f"   ‚ùå Critical error extracting from {source}: {e}")
            return []

    def _extract_json_from_response(self, response: str) -> str:
        """Extract JSON array from LLM response, handling wrapped content"""
        # Remove any markdown formatting
        response = re.sub(r"```json\s*", "", response)
        response = re.sub(r"```\s*", "", response)

        # Try to find JSON array
        json_match = re.search(r"\[\s*\{.*?\}\s*\]", response, re.DOTALL)
        if json_match:
            return json_match.group()

        # Try to find the start and end of array
        start_match = re.search(r"\[", response)
        end_match = re.search(r"\](?=[^}]*$)", response)

        if start_match and end_match:
            return response[start_match.start() : end_match.end()]

        # If response looks like it starts with array, try the whole thing
        if response.strip().startswith("["):
            return response.strip()

        return None

    def _remove_duplicate_events(self, events: List[Event]) -> List[Event]:
        """Remove duplicate events based on similarity (FIXED VERSION)"""
        if not events:
            return events

        unique_events = []
        seen_descriptions = []

        for event in events:
            # Check similarity with existing events
            is_duplicate = False
            for seen_desc in seen_descriptions:
                # Simple similarity check - if descriptions share many words
                event_words = set(event.description.lower().split())
                seen_words = set(seen_desc.lower().split())

                if len(event_words) > 0 and len(seen_words) > 0:
                    overlap = len(event_words.intersection(seen_words))
                    total = len(event_words.union(seen_words))
                    similarity = overlap / total if total > 0 else 0

                    if similarity > 0.6:  # 60% similarity threshold
                        is_duplicate = True
                        break

            if not is_duplicate:
                unique_events.append(event)
                seen_descriptions.append(event.description)

        print(f"   üîÑ Removed {len(events) - len(unique_events)} duplicate events")
        return unique_events

    def _supplement_with_owid_data(self, needed_count: int) -> List[Event]:
        """Supplement with Our World in Data if needed"""
        print(
            f"   üìä Supplementing with {needed_count} events from Our World in Data..."
        )

        try:
            owid_url = "https://raw.githubusercontent.com/owid/covid-19-data/master/public/data/owid-covid-data.csv"
            response = requests.get(owid_url, timeout=30)
            response.raise_for_status()

            # Save to temporary file
            with open("/tmp/owid_covid_data.csv", "wb") as f:
                f.write(response.content)

            df = pd.read_csv("/tmp/owid_covid_data.csv", nrows=1000)

            # Filter for interesting countries and significant events
            interesting_countries = [
                "United States",
                "India",
                "Germany",
                "Japan",
                "South Korea",
                "Brazil",
                "United Kingdom",
            ]
            df_filtered = df[df["location"].isin(interesting_countries)]
            df_filtered = df_filtered[
                pd.to_numeric(df_filtered["new_cases"], errors="coerce") > 5000
            ]

            additional_events = []
            for idx, (_, row) in enumerate(df_filtered.head(needed_count).iterrows()):
                new_cases = int(
                    pd.to_numeric(row.get("new_cases", 0), errors="coerce") or 0
                )
                total_cases = int(
                    pd.to_numeric(row.get("total_cases", 0), errors="coerce") or 0
                )
                location = row.get("location", "Unknown")

                event = Event(
                    entity_id=f"OWID_COVID_{idx:04d}",
                    event_type="significant_surge",
                    description=f"COVID-19 surge: {new_cases:,} new cases reported ({total_cases:,} total cases) in {location}",
                    timestamp=row.get("date", "2020-01-01"),
                    domain="covid",
                    location=location,
                    metadata={
                        "new_cases": new_cases,
                        "total_cases": total_cases,
                        "source": "Our World in Data - Significant Surges",
                        "event_significance": "surge",
                        "authentic_data": True,
                    },
                )
                additional_events.append(event)

            print(f"   ‚úÖ Added {len(additional_events)} OWID supplemental events")
            return additional_events

        except Exception as e:
            print(f"   ‚ö†Ô∏è OWID supplement failed: {e}")
            return []

    def _fallback_covid_events(self, target_count: int = None) -> List[Event]:
        """Fallback authentic COVID timeline with rich, varied events"""
        if target_count is None:
            target_count = DATASET_NO // 2

        authentic_events = [
            {
                "date": "2019-12-31",
                "event": "WHO receives reports of cluster of pneumonia cases in Wuhan, China with unknown cause",
                "location": "Wuhan, China",
                "type": "initial_outbreak",
                "cases": 27,
                "deaths": 0,
            },
            {
                "date": "2020-01-30",
                "event": "WHO declares COVID-19 outbreak a Public Health Emergency of International Concern (PHEIC)",
                "location": "Global",
                "type": "pheic_declaration",
                "cases": 7818,
                "deaths": 170,
            },
            {
                "date": "2020-02-11",
                "event": "WHO officially names the novel coronavirus disease COVID-19",
                "location": "Global",
                "type": "disease_naming",
                "cases": 43103,
                "deaths": 1018,
            },
            {
                "date": "2020-03-11",
                "event": "WHO declares COVID-19 a pandemic as global spread reaches 114 countries",
                "location": "Global",
                "type": "pandemic_declaration",
                "cases": 118319,
                "deaths": 4292,
            },
            {
                "date": "2020-03-09",
                "event": "Italy implements nationwide lockdown, becoming first European country with total restrictions",
                "location": "Italy",
                "type": "lockdown",
                "cases": 9172,
                "deaths": 463,
            },
            {
                "date": "2020-03-14",
                "event": "Spain declares national state of emergency and implements lockdown measures",
                "location": "Spain",
                "type": "emergency_declaration",
                "cases": 6391,
                "deaths": 195,
            },
            {
                "date": "2020-03-16",
                "event": "France announces nationwide lockdown starting March 17 for 15 days minimum",
                "location": "France",
                "type": "lockdown",
                "cases": 6633,
                "deaths": 148,
            },
            {
                "date": "2020-03-23",
                "event": "United Kingdom implements national lockdown after PM Johnson announces stay-at-home order",
                "location": "United Kingdom",
                "type": "lockdown",
                "cases": 6650,
                "deaths": 335,
            },
            {
                "date": "2020-04-02",
                "event": "Global COVID-19 cases exceed 1 million with widespread community transmission",
                "location": "Global",
                "type": "milestone",
                "cases": 1000000,
                "deaths": 51485,
            },
            {
                "date": "2020-05-01",
                "event": "United States reports over 1 million COVID-19 cases, highest in the world",
                "location": "United States",
                "type": "milestone",
                "cases": 1069424,
                "deaths": 62996,
            },
            {
                "date": "2020-06-15",
                "event": "Brazil becomes second country to exceed 1 million COVID-19 cases amid rising infections",
                "location": "Brazil",
                "type": "milestone",
                "cases": 1000000,
                "deaths": 48954,
            },
            {
                "date": "2020-11-09",
                "event": "Pfizer announces COVID-19 vaccine candidate is more than 90% effective in trials",
                "location": "Global",
                "type": "vaccine_breakthrough",
                "cases": 50000000,
                "deaths": 1250000,
            },
            {
                "date": "2020-12-08",
                "event": "United Kingdom becomes first country to approve Pfizer-BioNTech COVID-19 vaccine",
                "location": "United Kingdom",
                "type": "vaccine_approval",
                "cases": 1750000,
                "deaths": 62033,
            },
            {
                "date": "2020-12-14",
                "event": "First COVID-19 vaccination in the US administered to healthcare worker in New York",
                "location": "United States",
                "type": "vaccination_start",
                "cases": 16500000,
                "deaths": 301000,
            },
            {
                "date": "2021-01-06",
                "event": "WHO approves Pfizer-BioNTech COVID-19 vaccine for emergency use worldwide",
                "location": "Global",
                "type": "who_vaccine_approval",
                "cases": 86000000,
                "deaths": 1870000,
            },
        ]

        events = []
        # Take exactly target_count events, ensuring variety
        selected_events = (
            authentic_events[:target_count]
            if target_count <= len(authentic_events)
            else authentic_events
        )

        for i, item in enumerate(selected_events):
            event = Event(
                entity_id=f"AUTH_COVID_{i:04d}",
                event_type=item["type"],
                description=item["event"],
                timestamp=item["date"],
                domain="covid",
                location=item["location"],
                metadata={
                    "source": "Verified Historical Timeline",
                    "authentic": True,
                    "milestone": True,
                    "cases": item["cases"],
                    "deaths": item["deaths"],
                    "event_significance": "major"
                    if item["type"] in ["pandemic_declaration", "vaccine_breakthrough"]
                    else "significant",
                },
            )
            events.append(event)

        print(f"‚úÖ Generated {len(events)} rich authentic COVID events")
        print(f"   üìä Event types: {set(e.event_type for e in events)}")
        print(f"   üåç Locations: {set(e.location for e in events)}")
        return events

    def generate_ecommerce_data(self) -> List[Event]:
        """Generate realistic e-commerce data using LLM"""
        target_count = DATASET_NO // 2
        print(f"üõí Generating {target_count} e-commerce events using LLM...")

        try:
            prompt = f"""
            Generate exactly {target_count} realistic Amazon-style e-commerce customer journey events.
            Create logical customer behavior sequences (signup ‚Üí browse ‚Üí cart ‚Üí purchase ‚Üí review).
            
            Return JSON array with exactly {target_count} items and these fields:
            - customer_id (format: CUST_XXXXX)
            - event_type (signup, login, browse, add_to_cart, purchase, review, support_ticket, return)
            - timestamp (2023-01-01 to 2024-12-31, YYYY-MM-DD format)
            - product_category (electronics, books, clothing, home, sports, etc.)
            - description (brief event description)
            - order_value (number, 0 if not purchase)
            
            Example:
            [
              {{
                "customer_id": "CUST_00001",
                "event_type": "signup",
                "timestamp": "2023-06-15",
                "product_category": "general",
                "description": "Customer signed up for new account",
                "order_value": 0
              }}
            ]
            
            IMPORTANT: Return exactly {target_count} events, no more, no less.
            Make customers have realistic journeys over time.
            """

            response = openai.chat.completions.create(
                model="gpt-4",
                messages=[
                    {
                        "role": "system",
                        "content": "Generate realistic e-commerce data in JSON format. Follow the exact count requested.",
                    },
                    {"role": "user", "content": prompt},
                ],
                temperature=0.7,
                max_tokens=3000,
            )

            events_data = json.loads(response.choices[0].message.content)

            # Ensure we have exactly the right number of events
            events_data = events_data[:target_count]  # Truncate if too many

            events = []
            for i, event_data in enumerate(events_data):
                event = Event(
                    entity_id=f"ECOM_{i:04d}",
                    event_type=event_data.get("event_type", "unknown"),
                    description=event_data.get(
                        "description",
                        f"Customer activity: {event_data.get('event_type')}",
                    ),
                    timestamp=event_data.get("timestamp"),
                    domain="ecommerce",
                    location="Online Platform",
                    metadata={
                        "customer_id": event_data.get("customer_id"),
                        "product_category": event_data.get("product_category"),
                        "order_value": float(event_data.get("order_value", 0)),
                        "source": "LLM Generated E-commerce Data",
                    },
                )
                events.append(event)

            # If we got fewer events than expected, pad with fallback
            if len(events) < target_count:
                print(
                    f"   ‚ö†Ô∏è LLM generated {len(events)} events, padding to {target_count}"
                )
                fallback_events = self._fallback_ecommerce_events(
                    target_count - len(events)
                )
                events.extend(fallback_events)

            print(f"‚úÖ Generated exactly {len(events)} e-commerce events")

            # Show what LLM actually generated
            print(f"   üìã LLM generated events:")
            for event in events[:3]:  # Show first 3
                print(f"      ‚Ä¢ {event.description}")

            return events

        except Exception as e:
            print(f"‚ö†Ô∏è LLM generation failed: {e}")
            return self._fallback_ecommerce_events(target_count)

    def _fallback_ecommerce_events(self, target_count: int = None) -> List[Event]:
        """Fallback e-commerce events if LLM fails"""
        if target_count is None:
            target_count = DATASET_NO // 2

        customers = [f"CUST_{i:05d}" for i in range(1, min(target_count // 2 + 1, 10))]
        event_types = ["signup", "login", "browse", "add_to_cart", "purchase", "review"]
        categories = ["electronics", "books", "clothing", "home", "sports"]

        events = []
        for i in range(target_count):
            customer = customers[i % len(customers)]
            event_type = event_types[i % len(event_types)]
            category = random.choice(categories)

            event = Event(
                entity_id=f"ECOM_{i:04d}",
                event_type=event_type,
                description=f"{customer} performed {event_type} in {category}",
                timestamp=(
                    datetime.now() - timedelta(days=random.randint(1, 365))
                ).strftime("%Y-%m-%d"),
                domain="ecommerce",
                location="Online Platform",
                metadata={
                    "customer_id": customer,
                    "product_category": category,
                    "order_value": random.randint(10, 500)
                    if event_type == "purchase"
                    else 0,
                    "source": "Fallback E-commerce Data",
                },
            )
            events.append(event)

        print(f"‚úÖ Generated {len(events)} fallback e-commerce events")
        return events

    def setup_neo4j_database(self):
        """Setup Neo4j database - clear and create indexes"""
        print("üï∏Ô∏è Setting up Neo4j database...")

        with self.neo4j_driver.session() as session:
            # Clear existing data
            print("   üóëÔ∏è Clearing existing data...")
            session.run("MATCH (n) DETACH DELETE n")

            # Drop and recreate indexes
            session.run("DROP INDEX event_timestamp IF EXISTS")
            session.run("DROP INDEX event_domain IF EXISTS")
            session.run("DROP INDEX customer_id IF EXISTS")
            session.run("DROP INDEX location_name IF EXISTS")

            # Create new indexes
            session.run(
                "CREATE INDEX event_timestamp IF NOT EXISTS FOR (e:Event) ON (e.timestamp)"
            )
            session.run(
                "CREATE INDEX event_domain IF NOT EXISTS FOR (e:Event) ON (e.domain)"
            )
            session.run(
                "CREATE INDEX customer_id IF NOT EXISTS FOR (c:Customer) ON (c.customer_id)"
            )
            session.run(
                "CREATE INDEX location_name IF NOT EXISTS FOR (l:Location) ON (l.name)"
            )

        print("‚úÖ Neo4j database setup complete")

    def store_events_in_neo4j(self, events: List[Event]):
        """Store all events in Neo4j with relationships"""
        print(f"üíæ Storing {len(events)} events in Neo4j...")

        with self.neo4j_driver.session() as session:
            # Store events
            for event in events:
                if event.domain == "covid":
                    # COVID events
                    session.run(
                        """
                        CREATE (e:Event:CovidEvent {
                            entity_id: $entity_id,
                            event_type: $event_type,
                            description: $description,
                            timestamp: date($timestamp),
                            domain: $domain,
                            location: $location,
                            metadata: $metadata
                        })
                    """,
                        entity_id=event.entity_id,
                        event_type=event.event_type,
                        description=event.description,
                        timestamp=event.timestamp,
                        domain=event.domain,
                        location=event.location,
                        metadata=json.dumps(event.metadata),
                    )

                    # Create location relationship
                    session.run(
                        """
                        MERGE (l:Location {name: $location})
                        WITH l
                        MATCH (e:CovidEvent {entity_id: $entity_id})
                        CREATE (e)-[:OCCURRED_IN]->(l)
                    """,
                        location=event.location,
                        entity_id=event.entity_id,
                    )

                elif event.domain == "ecommerce":
                    # E-commerce events
                    customer_id = event.metadata.get("customer_id", "UNKNOWN")

                    session.run(
                        """
                        CREATE (e:Event:EcommerceEvent {
                            entity_id: $entity_id,
                            event_type: $event_type,
                            description: $description,
                            timestamp: date($timestamp),
                            domain: $domain,
                            customer_id: $customer_id,
                            product_category: $product_category,
                            order_value: $order_value,
                            metadata: $metadata
                        })
                    """,
                        entity_id=event.entity_id,
                        event_type=event.event_type,
                        description=event.description,
                        timestamp=event.timestamp,
                        domain=event.domain,
                        customer_id=customer_id,
                        product_category=event.metadata.get("product_category", ""),
                        order_value=event.metadata.get("order_value", 0),
                        metadata=json.dumps(event.metadata),
                    )

                    # Create customer relationship
                    session.run(
                        """
                        MERGE (c:Customer {customer_id: $customer_id})
                        WITH c
                        MATCH (e:EcommerceEvent {entity_id: $entity_id})
                        CREATE (c)-[:PERFORMED]->(e)
                    """,
                        customer_id=customer_id,
                        entity_id=event.entity_id,
                    )

            # Create temporal relationships
            print("   üîó Creating temporal relationships...")

            # COVID temporal chains (events in same location within 30 days)
            session.run("""
                MATCH (e1:CovidEvent), (e2:CovidEvent)
                WHERE e1.timestamp < e2.timestamp
                AND e1.location = e2.location
                AND duration.between(e1.timestamp, e2.timestamp).days <= 30
                CREATE (e1)-[:FOLLOWED_BY {
                    days_between: duration.between(e1.timestamp, e2.timestamp).days,
                    relationship_type: 'covid_sequence'
                }]->(e2)
            """)

            # Customer journey chains (same customer within 14 days)
            session.run("""
                MATCH (e1:EcommerceEvent), (e2:EcommerceEvent)
                WHERE e1.customer_id = e2.customer_id
                AND e1.timestamp < e2.timestamp
                AND duration.between(e1.timestamp, e2.timestamp).days <= 14
                CREATE (e1)-[:FOLLOWED_BY {
                    days_between: duration.between(e1.timestamp, e2.timestamp).days,
                    relationship_type: 'customer_journey'
                }]->(e2)
            """)

        print("‚úÖ All events and relationships stored in Neo4j")

    def verify_data_integrity(self):
        """Verify stored data"""
        print("üîç Verifying data integrity...")

        with self.neo4j_driver.session() as session:
            # Count events by type
            result = session.run("""
                MATCH (e:Event)
                RETURN labels(e)[1] as event_type, count(e) as count
            """)

            event_counts = {record["event_type"]: record["count"] for record in result}

            # Count relationships
            result = session.run(
                "MATCH ()-[r:FOLLOWED_BY]->() RETURN count(r) as count"
            )
            relationship_count = result.single()["count"]

            # Count locations and customers
            result = session.run("MATCH (l:Location) RETURN count(l) as count")
            location_count = result.single()["count"]

            result = session.run("MATCH (c:Customer) RETURN count(c) as count")
            customer_count = result.single()["count"]

        total_events = sum(event_counts.values())

        print(f"""
üìä DATA VERIFICATION RESULTS:
‚Ä¢ COVID Events: {event_counts.get("CovidEvent", 0)}
‚Ä¢ E-commerce Events: {event_counts.get("EcommerceEvent", 0)}
‚Ä¢ Temporal Relationships: {relationship_count}
‚Ä¢ Locations: {location_count}
‚Ä¢ Customers: {customer_count}
‚Ä¢ Total Events: {total_events}
‚Ä¢ Expected Events: {DATASET_NO}
        """)

        # More flexible verification - allow ¬±1 event difference
        if abs(total_events - DATASET_NO) <= 1:
            print("‚úÖ Data integrity verification PASSED")
            return True
        else:
            print(f"‚ö†Ô∏è Data count mismatch: expected {DATASET_NO}, got {total_events}")

            # If we have events and relationships, it's still usable
            if total_events > 0 and relationship_count > 0:
                print("‚úÖ Data integrity verification PASSED (with count variance)")
                return True
            else:
                print("‚ùå Data integrity verification FAILED")
                return False

    def close(self):
        """Close Neo4j connection"""
        if self.neo4j_driver:
            self.neo4j_driver.close()


def main():
    """Main data generation pipeline"""
    # Determine COVID data approach
    if USE_AUTHENTIC_SCRAPING:
        covid_approach = "Enhanced Multi-Government Scraping + LLM Structuring"
    elif USE_LLM_FOR_COVID:
        covid_approach = "Pure LLM Generated"
    else:
        covid_approach = "Verified Historical Timeline"

    print(f"üöÄ Starting Enhanced Data Generation Pipeline")
    print(f"üìä Dataset Size: {DATASET_NO} events")
    print(f"üóÑÔ∏è Storage: Neo4j at {NEO4J_URI}")
    print(f"ü¶† COVID Data: {covid_approach}")
    print(f"üõí E-commerce Data: LLM Generated")

    # Validate environment
    if not OPENAI_API_KEY:
        print("‚ùå Missing OPENAI_API_KEY environment variable")
        return
    if not NEO4J_PASSWORD:
        print("‚ùå Missing NEO4J_PASSWORD environment variable")
        return

    generator = DataGenerator()

    try:
        # 1. Setup Neo4j database
        generator.setup_neo4j_database()

        # 2. Generate COVID data (multiple approaches available)
        if USE_AUTHENTIC_SCRAPING:
            print("üåê Using enhanced multi-government scraping + LLM structuring...")
            covid_events = generator.scrape_authentic_covid_sources(DATASET_NO // 2)

        elif USE_LLM_FOR_COVID:
            print("ü§ñ Using pure LLM for COVID data generation...")
            covid_events = generator.fetch_authentic_covid_data()  # This now uses LLM
        else:
            print("üìö Using verified authentic COVID timeline...")
            covid_events = generator._fallback_covid_events(DATASET_NO // 2)

        # 3. Generate e-commerce data (LLM)
        ecommerce_events = generator.generate_ecommerce_data()

        # 4. Combine all events
        all_events = covid_events + ecommerce_events
        print(f"üìà Total events generated: {len(all_events)}")

        # 5. Store in Neo4j
        generator.store_events_in_neo4j(all_events)

        # 6. Verify data integrity
        success = generator.verify_data_integrity()

        if success:
            print(f"""
üéâ ENHANCED DATA GENERATION COMPLETE!
üìä Successfully generated and stored {len(all_events)} events from authentic government sources
üï∏Ô∏è Neo4j contains temporal graph with relationships
‚úÖ Ready for ground truth generation (Script 2)

Sources used for COVID data:
‚Ä¢ CDC COVID-19 Timeline
‚Ä¢ WHO COVID-19 Information
‚Ä¢ ECDC COVID-19 Data
‚Ä¢ Our World in Data
‚Ä¢ Verified Historical Timeline (fallback)

Next steps:
1. Run: python generate_ground_truth.py
2. Run: python run_evaluation.py
            """)
        else:
            print("‚ùå Data generation failed verification")

    except Exception as e:
        print(f"‚ùå Pipeline failed: {e}")
        import traceback

        traceback.print_exc()

    finally:
        generator.close()


if __name__ == "__main__":
    main()
